{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415625de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5af1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ready', 'take', 'first', 'step', 'towards', 'prosperous', 'future', 'sign', 'today', 'gain', 'instant', 'access', 'comprehensive', 'suite', 'tool', 'resource', 'community', 'support', 'start', 'exploring', 'opportunity', 'within', 'capital', 'market', 'unlock', 'potential', 'wealth', 'creation', 'financial', 'security']\n"
     ]
    }
   ],
   "source": [
    "# Ques 1:  Implement Lemmatization: Replace the stemming process with lemmatization using WordNetLemmatizer.\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Sample text\n",
    "text = \"Ready to take the first step towards a prosperous future? Sign up today and gain instant access to our comprehensive suite of tools, resources, and community support. Start exploring the opportunities within the capital markets and unlock your potential for wealth creation and financial security.\"\n",
    "\n",
    "# Preprocess the sample text\n",
    "preprocessed_tokens = preprocess_text(text)\n",
    "print(preprocessed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a27004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['código', 'proporcionado', 'demuestra', 'cómo', 'utilizar', 'countvectorizer', 'biblioteca', 'scikitlearn', 'realizar', 'preprocesamiento', 'texto', 'convertir', 'colección', 'documentos', 'texto', 'matriz', 'recuentos', 'token', 'conocida', 'modelo', 'bagofwords', 'bow', 'aquí', 'explicación', 'cada', 'paso']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ques 2: Handle Different Languages:Modify the code to preprocess text in a different language using the appropriate stopwords and tokenizers.\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize the lemmatizer and Spanish stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "spanish_stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def preprocess_text_spanish(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and perform lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in spanish_stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Sample Spanish text\n",
    "text_spanish = \"El código proporcionado demuestra cómo utilizar el CountVectorizer de la biblioteca scikit-learn para realizar el preprocesamiento de texto y convertir una colección de documentos de texto en una matriz de recuentos de tokens, también conocida como el modelo Bag-of-Words (BoW). Aquí hay una explicación de cada paso.\"\n",
    "\n",
    "# Preprocess the sample Spanish text\n",
    "preprocessed_tokens_spanish = preprocess_text_spanish(text_spanish)\n",
    "print(preprocessed_tokens_spanish)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cfa4b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Feature Names: ['brown' 'dog' 'fox' 'jump' 'jumped' 'lazy' 'quick' 'quickly']\n",
      "CountVectorizer Document Matrix (BoW):\n",
      "[[1 1 1 0 1 1 1 0]\n",
      " [0 1 0 1 0 1 0 1]\n",
      " [0 1 1 0 0 1 1 0]]\n",
      "\n",
      "TfidfVectorizer Feature Names: ['brown' 'dog' 'fox' 'jump' 'jumped' 'lazy' 'quick' 'quickly']\n",
      "TfidfVectorizer Document Matrix:\n",
      "[[0.50935267 0.30083189 0.38737583 0.         0.50935267 0.30083189\n",
      "  0.38737583 0.        ]\n",
      " [0.         0.35959372 0.         0.6088451  0.         0.35959372\n",
      "  0.         0.6088451 ]\n",
      " [0.         0.43370786 0.55847784 0.         0.         0.43370786\n",
      "  0.55847784 0.        ]]\n",
      "\n",
      "HashingVectorizer Document Matrix:\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.70710678  0.70710678  0.        ]\n",
      " [ 0.          0.         -0.5        -0.5         0.          0.\n",
      "   0.         -0.5         0.          0.5       ]\n",
      " [ 0.          0.          0.         -0.70710678  0.          0.\n",
      "   0.         -0.70710678  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Ques 3: Explore Other Vectorizers:* Explore Other Vectorizers: Try using HashingVectorizer and compare it with CountVectorizer and TfidfVectorizer.\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog quickly.\",\n",
    "    \"The dog is lazy but the fox is quick.\"\n",
    "]\n",
    "\n",
    "# Initialize Vectorizers\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "hashing_vectorizer = HashingVectorizer(n_features=10, stop_words='english')\n",
    "\n",
    "# Fit and transform the documents\n",
    "X_count = count_vectorizer.fit_transform(documents)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "X_hash = hashing_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names and converted document matrix for CountVectorizer and TfidfVectorizer\n",
    "feature_names_count = count_vectorizer.get_feature_names_out()\n",
    "document_matrix_count = X_count.toarray()\n",
    "\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "document_matrix_tfidf = X_tfidf.toarray()\n",
    "\n",
    "# Document matrix for HashingVectorizer (HashingVectorizer doesn't provide feature names)\n",
    "document_matrix_hash = X_hash.toarray()\n",
    "\n",
    "print(\"CountVectorizer Feature Names:\", feature_names_count)\n",
    "print(\"CountVectorizer Document Matrix (BoW):\")\n",
    "print(document_matrix_count)\n",
    "\n",
    "print(\"\\nTfidfVectorizer Feature Names:\", feature_names_tfidf)\n",
    "print(\"TfidfVectorizer Document Matrix:\")\n",
    "print(document_matrix_tfidf)\n",
    "\n",
    "print(\"\\nHashingVectorizer Document Matrix:\")\n",
    "print(document_matrix_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db885d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
